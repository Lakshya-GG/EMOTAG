{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install emoji --upgrade\n"
      ],
      "metadata": {
        "id": "-FrM12WxE7Gb",
        "outputId": "9921783e-930b-4d29-dc07-0ad935c81f69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=4d32b7d945eb6e6b407d32b42ba27af0b763605c97c2e45747fa61a2ec3a3a79\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/62/9e/a6b27a681abcde69970dbc0326ff51955f3beac72f15696984\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PWWxQIauEPQU"
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SPUDSCA2EPQW"
      },
      "outputs": [],
      "source": [
        "#emoji.EMOJI_UNICODE\n",
        "#this shows a dictionary in which each emoji is matched to some keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "drYCMhCjEPQX"
      },
      "outputs": [],
      "source": [
        "# we will use only 5 emojis(0-4) because our training data has examples of only these emojies\n",
        "emoji_dictionary={'0':'\\u2764\\uFE0F', #heart prints a  heart\n",
        "                 '1':':baseball:',\n",
        "                 '2':':grinning_face_with_big_eyes:',\n",
        "                 '3':':disappointed_face:',\n",
        "                 '4':':fork_and_knife:',\n",
        "                 '5':':hundred_points:',\n",
        "                  '6':':fire:',\n",
        "                  '7':':face_blowing_a_kiss:',\n",
        "                  '8':':chestnut:',\n",
        "                  '9':':flexed_biceps:'\n",
        "                 }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I21PvmR2EPQY",
        "outputId": "bedf6cb0-644f-4443-c020-78e58daa1e6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❤️\n",
            "⚾\n",
            "😃\n",
            "😞\n",
            "🍴\n",
            "💯\n",
            "🔥\n",
            "😘\n",
            "🌰\n",
            "💪\n"
          ]
        }
      ],
      "source": [
        "#emojize function is used to print the emojis using given keyword related to it(if present)\n",
        "for e in emoji_dictionary.values():\n",
        "    print(emoji.emojize(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o1h4HsNEPQZ"
      },
      "outputs": [],
      "source": [
        "#Load The training and testing data\n",
        "import pandas as pd\n",
        "train=pd.read_csv('train_emoji.csv',header=None)\n",
        "test=pd.read_csv('test_emoji.csv',header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egSdxVU3EPQZ",
        "outputId": "88462718-ac4a-4fa1-c6e3-0a2cb7f64a00"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I want to eat\\t</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>he did not answer\\t</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>he got a raise\\t</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>she got me a present\\t</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ha ha ha it was so funny\\t</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            0  1\n",
              "0             I want to eat\\t  4\n",
              "1         he did not answer\\t  3\n",
              "2            he got a raise\\t  2\n",
              "3      she got me a present\\t  0\n",
              "4  ha ha ha it was so funny\\t  2"
            ]
          },
          "execution_count": 260,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test.head()\n",
        "#labels have been given to the emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwE5ZSc8EPQZ"
      },
      "outputs": [],
      "source": [
        "xtrain=train[0]\n",
        "ytrain=train[1]\n",
        "#first column is sentences and second column is labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBVqllfzEPQa",
        "outputId": "2ebb033b-2fc4-495c-d476-dd993f5f7b95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "never talk to me again 😞\n",
            "I am proud of your achievements 😃\n",
            "It is the worst day in my life 😞\n",
            "Miss you so much ❤️\n",
            "food is life 🍴\n",
            "I love you mum ❤️\n",
            "Stop saying bullshit 😞\n",
            "congratulations on your acceptance 😃\n",
            "The assignment is too long  😞\n",
            "I want to go play ⚾\n",
            "she did not answer my text  😞\n",
            "Your stupidity has no limit 😞\n",
            "how many points did he score ⚾\n",
            "my algorithm performs poorly 😞\n",
            "I got approved 😃\n",
            "Stop shouting at me 😞\n",
            "Sounds like a fun plan ha ha 😃\n",
            "no one likes him 😞\n",
            "the game just finished ⚾\n",
            "I will celebrate soon 😃\n",
            "So sad you are not coming 😞\n",
            "She is my dearest love ❤️\n",
            "Good job 😃\n",
            "It was funny lol 😃\n",
            "candy is life  😃\n",
            "The chicago cubs won again ⚾\n",
            "I am hungry 🍴\n",
            "I am so excited to see you after so long 😃\n",
            "you did well on you exam 😃\n",
            "lets brunch some day 🍴\n",
            "he is so cute ❤️\n",
            "How dare you ask that 😞\n",
            "do you want to join me for dinner  🍴\n",
            "I said yes 😃\n",
            "she is attractive ❤️\n",
            "you suck 😞\n",
            "she smiles a lot 😃\n",
            "he is laughing 😃\n",
            "she takes forever to get ready  😞\n",
            "French macaroon is so tasty 🍴\n",
            "we made it 😃\n",
            "I am excited 😃\n",
            "I adore my dogs ❤️\n",
            "Congratulations 😃\n",
            "this girl was mean 😞\n",
            "you two are cute ❤️\n",
            "my code is working but the grader gave me zero 😞\n",
            "this joke is killing me haha 😃\n",
            "do you like pizza  🍴\n",
            "you got a down grade 😞\n",
            "I missed you ❤️\n",
            "I think I will end up alone 😞\n",
            "I got humiliated by my sister 😞\n",
            "you are awful 😞\n",
            "I cooked meat 🍴\n",
            "This is so funny 😃\n",
            "lets exercise ⚾\n",
            "he is the best player ⚾\n",
            "I am going to the stadium ⚾\n",
            "You are incredibly intelligent and talented 😃\n",
            "Stop shouting at me 😞\n",
            "Who is your favorite player ⚾\n",
            "I like you a lot ❤️\n",
            "i miss him ❤️\n",
            "my dog just had a few puppies ❤️\n",
            "I hate him 😞\n",
            "I want chinese food 🍴\n",
            "cookies are good 🍴\n",
            "her smile is so charming 😃\n",
            "Bravo for the announcement it got a lot of traction 😃\n",
            "she plays baseball ⚾\n",
            "he did an amazing job 😃\n",
            "The baby is adorable ❤️\n",
            "I was waiting for her for two hours  😞\n",
            "funny 😃\n",
            "I like it when people smile 😃\n",
            "I love dogs ❤️\n",
            "they are so kind and friendly ❤️\n",
            "So bad that you cannot come with us 😞\n",
            "he likes baseball ⚾\n",
            "I am so impressed by your dedication to this project 😃\n",
            "I am at the baseball game ⚾\n",
            "Bravo 😃\n",
            "What a fun moment 😃\n",
            "I want to have sushi for dinner 🍴\n",
            "I am very disappointed 😞\n",
            "he can not do anything 😞\n",
            "lol 😃\n",
            "Lets have food together 🍴\n",
            "she is so cute ❤️\n",
            "miss you my dear ❤️\n",
            "I am looking for a date ❤️\n",
            "I am frustrated 😞\n",
            "I lost my wallet 😞\n",
            "you failed the midterm 😞\n",
            "ha ha ha it was so funny 😃\n",
            "Do you want to give me a hug ❤️\n",
            "who is playing in the final ⚾\n",
            "she is happy 😃\n",
            "You are not qualified for this position 😞\n",
            "I love my dad ❤️\n",
            "this guy was such a joke 😃\n",
            "Good joke 😃\n",
            "This specialization is great 😃\n",
            "you could not solve it 😞\n",
            "I am so happy for you 😃\n",
            "Congrats on the new job 😃\n",
            "I am proud of you forever 😃\n",
            "I want to eat 🍴\n",
            "That catcher sucks  ⚾\n",
            "The first base man got the ball ⚾\n",
            "this is bad 😞\n",
            "you did not do your homework 😞\n",
            "I will have a cheese cake 🍴\n",
            "do you have a ball ⚾\n",
            "the lectures are great though  😃\n",
            "Are you down for baseball this afternoon ⚾\n",
            "what are the rules of the game ⚾\n",
            "I am always working 😞\n",
            "where is the stadium ⚾\n",
            "She is the cutest person I have ever seen ❤️\n",
            "vegetables are healthy 🍴\n",
            "he is handsome ❤️\n",
            "too bad that you were not here 😞\n",
            "you are a loser 😞\n",
            "I love indian food 🍴\n",
            "Who is down for a restaurant 🍴\n",
            "he had to make a home run ⚾\n",
            "I am ordering food 🍴\n",
            "What is wrong with you 😞\n",
            "I love you ❤️\n",
            "great job 😃\n"
          ]
        }
      ],
      "source": [
        "#We try to print the sentences and relevant emojis\n",
        "for i in range(ytrain.shape[0]):\n",
        "    print(xtrain[i],end=' ')\n",
        "    print(emoji.emojize(emoji_dictionary[str(ytrain[i])]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znarreA4EPQa"
      },
      "outputs": [],
      "source": [
        "xtest=test[0]\n",
        "ytest=test[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfAe8J5MEPQa"
      },
      "outputs": [],
      "source": [
        "#We use a pre trained embedding layer i.e Glove Vector\n",
        "#it has 6 billions words and 50 sized vector for each word\n",
        "f=open('glove.6B.50d.txt',encoding='utf-8')\n",
        "#from this text file we create our won dictionary of words and their corresponding vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_rIjQIqEPQa"
      },
      "outputs": [],
      "source": [
        "embeddings_index={}\n",
        "#initialise and empty dict.\n",
        "for line in f:\n",
        "    values=line.split()\n",
        "    #divide the lines over space\n",
        "    #first word is the word reqd and rest is its vector\n",
        "    word=values[0]\n",
        "    coeff=np.asarray(values[1:],dtype='float')\n",
        "    #print(word,coeff)\n",
        "    embeddings_index[word]=coeff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D31EUajqEPQb"
      },
      "outputs": [],
      "source": [
        "#we have got vectors corresponding to each word\n",
        "#now we will get vectors corresponding to sentences\n",
        "def embedding_output(X):\n",
        "    #fix max num of words in a sentence =10\n",
        "    maxlen=10\n",
        "    emb_dim=50\n",
        "    #dimesnion of one vector\n",
        "    embedding_out=np.zeros((X.shape[0],maxlen,emb_dim))\n",
        "    #embedding_out[i][j] represents vector of jth word in ith line\n",
        "    for i in range(X.shape[0]):\n",
        "        words=X[i].split()\n",
        "        #extract the words from each line\n",
        "        for j in range(len(words)):\n",
        "            try :\n",
        "                #then assign its corresponding vector\n",
        "                embedding_out[i][j]=embeddings_index[words[j].lower()]\n",
        "            except:\n",
        "                #incase the word is not present in our dict.\n",
        "                embedding_out[i][j]=np.zeros((50,))\n",
        "    return embedding_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5JUEyVdEPQb"
      },
      "outputs": [],
      "source": [
        "embedding_matrix_train=embedding_output(xtrain)\n",
        "embedding_matrix_test=embedding_output(xtest)\n",
        "#convert the training and testing data to embedded layer o/p format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5-4XvsNEPQb"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Dropout,Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UExd30jKEPQc"
      },
      "outputs": [],
      "source": [
        "#initialise the model\n",
        "model=Sequential()\n",
        "#size of activation vector is kept 64\n",
        "#return sequences=true means that o/p of this LSTM will be fed to the next LSTM Layer\n",
        "model.add(LSTM(64,input_shape=(10,50),return_sequences=True))\n",
        "#Dropout Layer\n",
        "model.add(Dropout(0.5))\n",
        "#Since we have only 2 LSTM Layer so last layer will give the output to the classification layer(i.e dense layer)\n",
        "model.add(LSTM(64,return_sequences=False))\n",
        "model.add(Dropout(0.5))\n",
        "#no of class=5\n",
        "model.add(Dense(5,activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQVZnLuwEPQc"
      },
      "outputs": [],
      "source": [
        "#loss is categorical cross entropy as it is a multi class classification\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re6-IUqWEPQc"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "#model need to be fed in one hot notation\n",
        "ytrain=to_categorical(ytrain,num_classes=5)\n",
        "ytest=to_categorical(ytest,num_classes=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD30YH4MEPQd",
        "outputId": "5409f0b5-a55a-475d-c52a-e88d9f60d36b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 4s 770ms/step - loss: 1.5848 - accuracy: 0.2421 - val_loss: 1.6167 - val_accuracy: 0.2222\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.61667, saving model to best_model.h5\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 1.5535 - accuracy: 0.3010 - val_loss: 1.6225 - val_accuracy: 0.2222\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 1.61667\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 1.5238 - accuracy: 0.3506 - val_loss: 1.6337 - val_accuracy: 0.1852\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.61667\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 1.5041 - accuracy: 0.3622 - val_loss: 1.6485 - val_accuracy: 0.2593\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.61667\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 1.4840 - accuracy: 0.3801 - val_loss: 1.6581 - val_accuracy: 0.2222\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.61667\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 1.4512 - accuracy: 0.4293 - val_loss: 1.6650 - val_accuracy: 0.2222\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.61667\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 1.4673 - accuracy: 0.3790 - val_loss: 1.6564 - val_accuracy: 0.2222\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.61667\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 1.4137 - accuracy: 0.4465 - val_loss: 1.6440 - val_accuracy: 0.2593\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.61667\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 1.3626 - accuracy: 0.4662 - val_loss: 1.6316 - val_accuracy: 0.2593\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.61667\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 1.4082 - accuracy: 0.3697 - val_loss: 1.6040 - val_accuracy: 0.2222\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.61667 to 1.60403, saving model to best_model.h5\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 1.3429 - accuracy: 0.4315 - val_loss: 1.5706 - val_accuracy: 0.2593\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.60403 to 1.57062, saving model to best_model.h5\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 1.2936 - accuracy: 0.4864 - val_loss: 1.5315 - val_accuracy: 0.2593\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.57062 to 1.53149, saving model to best_model.h5\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 1.2542 - accuracy: 0.4801 - val_loss: 1.4905 - val_accuracy: 0.2963\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.53149 to 1.49051, saving model to best_model.h5\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 1.1903 - accuracy: 0.5904 - val_loss: 1.4550 - val_accuracy: 0.2593\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.49051 to 1.45499, saving model to best_model.h5\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 1.1788 - accuracy: 0.6494 - val_loss: 1.4200 - val_accuracy: 0.2963\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.45499 to 1.42001, saving model to best_model.h5\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 1.0510 - accuracy: 0.6557 - val_loss: 1.3931 - val_accuracy: 0.3704\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.42001 to 1.39311, saving model to best_model.h5\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 1.0296 - accuracy: 0.6852 - val_loss: 1.3594 - val_accuracy: 0.4444\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.39311 to 1.35939, saving model to best_model.h5\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.8720 - accuracy: 0.6736 - val_loss: 1.3543 - val_accuracy: 0.4444\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.35939 to 1.35425, saving model to best_model.h5\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.8900 - accuracy: 0.6770 - val_loss: 1.3175 - val_accuracy: 0.4444\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.35425 to 1.31748, saving model to best_model.h5\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.7982 - accuracy: 0.7412 - val_loss: 1.2616 - val_accuracy: 0.3704\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.31748 to 1.26162, saving model to best_model.h5\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.7061 - accuracy: 0.7620 - val_loss: 1.2093 - val_accuracy: 0.4074\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.26162 to 1.20932, saving model to best_model.h5\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.6490 - accuracy: 0.7851 - val_loss: 1.1821 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.20932 to 1.18214, saving model to best_model.h5\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.5490 - accuracy: 0.8105 - val_loss: 1.1259 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.18214 to 1.12594, saving model to best_model.h5\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.5944 - accuracy: 0.7770 - val_loss: 1.1476 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.12594\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.5210 - accuracy: 0.8296 - val_loss: 1.0402 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.12594 to 1.04023, saving model to best_model.h5\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.4730 - accuracy: 0.8579 - val_loss: 0.9097 - val_accuracy: 0.6296\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.04023 to 0.90968, saving model to best_model.h5\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.4223 - accuracy: 0.8862 - val_loss: 0.9901 - val_accuracy: 0.5926\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.90968\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.4033 - accuracy: 0.9000 - val_loss: 0.9077 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.90968 to 0.90770, saving model to best_model.h5\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.3131 - accuracy: 0.9220 - val_loss: 1.0228 - val_accuracy: 0.6296\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.90770\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.4248 - accuracy: 0.8590 - val_loss: 1.0872 - val_accuracy: 0.6296\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.90770\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.3068 - accuracy: 0.9272 - val_loss: 1.0585 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.90770\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.3397 - accuracy: 0.9116 - val_loss: 0.9909 - val_accuracy: 0.6296\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.90770\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.2319 - accuracy: 0.9567 - val_loss: 1.0531 - val_accuracy: 0.6296\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.90770\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.3173 - accuracy: 0.8781 - val_loss: 1.0312 - val_accuracy: 0.6296\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.90770\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2614 - accuracy: 0.9284 - val_loss: 1.0227 - val_accuracy: 0.7407\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.90770\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2575 - accuracy: 0.9336 - val_loss: 1.1114 - val_accuracy: 0.7037\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.90770\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.3463 - accuracy: 0.8821 - val_loss: 0.9317 - val_accuracy: 0.7037\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.90770\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.1636 - accuracy: 0.9642 - val_loss: 0.9251 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.90770\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "#use callbacks like checkpoint and earlystopping for best val acuracy\n",
        "checkpoint=ModelCheckpoint(\"best_model.h5\",monitor='val_loss',verbose=True,save_best_only=True)\n",
        "earlystop=EarlyStopping(monitor='val_loss',patience=10)\n",
        "hist=model.fit(embedding_matrix_train,ytrain,epochs=100,batch_size=64,shuffle=True,validation_split=0.2,callbacks=[checkpoint,earlystop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nefv96ELEPQd",
        "outputId": "a62080f4-7643-4e91-c9a7-5386b69be38a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\DEVIL_Pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ]
        }
      ],
      "source": [
        "preds=model.predict_classes(embedding_matrix_test)\n",
        "#predict the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE81yRxtEPQd",
        "outputId": "e6625d59-bf45-4363-fa33-37f272c3781e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 5ms/step - loss: 1.2614 - accuracy: 0.6071\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.2613991498947144, 0.6071428656578064]"
            ]
          },
          "execution_count": 275,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(embedding_matrix_test,ytest)\n",
        "#check the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRKkgDUkEPQd",
        "outputId": "ad64e803-00c3-4a6e-8288-55f7019a2819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I want to eat\t 🍴\n",
            "he did not answer\t 😞\n",
            "he got a raise\t ⚾\n",
            "she got me a present\t ❤️\n",
            "ha ha ha it was so funny\t 😃\n",
            "he is a good friend\t 😃\n",
            "I am upset\t ⚾\n",
            "We had such a lovely dinner tonight\t 😃\n",
            "where is the food\t 🍴\n",
            "Stop making this joke ha ha ha\t 😃\n",
            "where is the ball\t ⚾\n",
            "work is hard\t 😃\n",
            "This girl is messing with me\t ❤️\n",
            "are you serious ha ha\t 😞\n",
            "Let us go play baseball\t ⚾\n",
            "This stupid grader is not working \t 😞\n",
            "work is horrible\t 😃\n",
            "Congratulation for having a baby\t 😃\n",
            "stop messing around\t 😞\n",
            "any suggestions for dinner\t 😃\n",
            "I love taking breaks\t ❤️\n",
            "you brighten my day\t ❤️\n",
            "I boiled rice\t 🍴\n",
            "she is a bully\t ❤️\n",
            "Why are you feeling bad\t 😞\n",
            "I am upset\t ⚾\n",
            "I worked during my birthday\t 😃\n",
            "My grandmother is the love of my life\t ❤️\n",
            "enjoy your break\t ❤️\n",
            "valentine day is near\t 😃\n"
          ]
        }
      ],
      "source": [
        "for i in range(30):\n",
        "    print(xtest[i],emoji.emojize(emoji_dictionary[str(preds[i])]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAF5EmvWEPQe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}